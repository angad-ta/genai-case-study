{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zbzKE8XCIAnH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702024343545,"user_tz":-330,"elapsed":15032,"user":{"displayName":"Nichol Dsouza","userId":"00331130812712411638"}},"outputId":"8bbfd839-b050-4c62-8714-115c32ed171d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m886.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q evaluate transformers accelerate bitsandbytes peft"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmOLBy4ORnV3"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","from evaluate import load\n","from datasets import load_from_disk\n","from peft import PeftModel, PeftConfig\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VVkq6R3WS76j"},"outputs":[],"source":["model_name_or_path = \"google/flan-t5-large\"\n","tokenizer_name_or_path = \"google/flan-t5-large\"\n","peft_model_path = \"/content/drive/MyDrive/Colab Notebooks/Table to insights/Flan/prefix_tuned_model\"\n","text_column = \"prompt\"\n","label_column = \"label\"\n","max_length = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YypbIHnfTOXY"},"outputs":[],"source":["# Specify the directory where the dataset is saved\n","saved_directory = \"/content/drive/MyDrive/Colab Notebooks/Table to insights/Data/Analytical Datset\"\n","\n","# Load the dataset from the specified directory\n","dataset = load_from_disk(saved_directory)\n","dataset = dataset['validation']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_sbVh-IcTWOm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702024363446,"user_tz":-330,"elapsed":16,"user":{"displayName":"Nichol Dsouza","userId":"00331130812712411638"}},"outputId":"b652ee58-313a-46ab-da0d-9cf35482e484"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['table', 'prompt', 'label'],\n","    num_rows: 129\n","})"]},"metadata":{},"execution_count":5}],"source":["dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NjkAh4bTZEW"},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","def generate_summary(model, tokenizer, query):\n","    input_ids = tokenizer.encode(query, return_tensors=\"pt\", max_length=max_length,padding=\"max_length\" ,truncation=True)\n","    summary_ids = model.generate(input_ids, max_length=150, num_beams=4, length_penalty=2.0, early_stopping=True)\n","    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","    return summary\n","\n","def summarize_example(example, original_model, original_tokenizer, peft_tuned_model, peft_tuned_tokenizer):\n","    prompt = example[text_column]\n","    label = example[label_column]\n","\n","    # Generate summaries using the original model\n","    original_model_summary = generate_summary(original_model, original_tokenizer, prompt)\n","\n","    # Generate summaries using the PEFT tuned model\n","    tuned_summary = generate_summary(peft_tuned_model, peft_tuned_tokenizer, prompt)\n","\n","    return {\"prompt\": prompt, \"label\": label, \"original_model_summary\": original_model_summary, \"tuned_summary\": tuned_summary}\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"xyEMi5l_hM0P","executionInfo":{"status":"error","timestamp":1702025325020,"user_tz":-330,"elapsed":25520,"user":{"displayName":"Nichol Dsouza","userId":"00331130812712411638"}},"outputId":"90a17496-3002-4662-bea9-7c2c2ed47b8d"},"outputs":[{"output_type":"error","ename":"HFValidationError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-6690b5c39379>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the PEFT tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpeft_tuned_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuned_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Use map to apply the summarization function to each example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    489\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         ):\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/Colab Notebooks/Table to insights/Flan/Default run/adapter_config.json'. Use `repo_type` argument if needed."]}],"source":["# Load the original model\n","original_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","original_model = AutoModelForSeq2SeqLM.from_pretrained(tokenizer_name_or_path,load_in_8bit=True,device_map=\"auto\")\n","\n","# Load the PEFT tuned model\n","config = PeftConfig.from_pretrained(peft_model_path)\n","peft_base_model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n","peft_tuned_model = PeftModel.from_pretrained(peft_base_model, peft_model_path)\n","\n","# Use map to apply the summarization function to each example\n","summarized_dataset = dataset.map(lambda example: summarize_example(example, original_model, original_tokenizer, peft_tuned_model, original_tokenizer))\n","\n","# Display or save the results as a DataFrame\n","df = pd.DataFrame(summarized_dataset)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1XLXivNzH0EUy3AB0zisaX3zduYrl6HK8","authorship_tag":"ABX9TyMYoA2HeWZguXVmtiv3N3XF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}